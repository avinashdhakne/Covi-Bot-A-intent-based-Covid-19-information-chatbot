{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8D3gbnQZYH"
      },
      "outputs": [],
      "source": [
        "# read the json file\n",
        "import json\n",
        "with open('final_data.json') as file:\n",
        "    intents = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iAeoMYr974i"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REq16v-oQoiV"
      },
      "outputs": [],
      "source": [
        "# import the requiremnets\n",
        "import pickle\n",
        "vect = pickle.load(open(\"vectoriser.pkl\",\"rb\"))\n",
        "tags = pickle.load(open(\"tag.pkl\",\"rb\"))\n",
        "x_final = pickle.load(open(\"x_final.pkl\",\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mq8SVzrQteb"
      },
      "outputs": [],
      "source": [
        "# load the Chatbot model\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('Ch_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2neWlgvFQwqL",
        "outputId": "0d74e3cb-cbbb-4e6f-c739-f8ed21d29e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import required libraries\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# import stemmer to conver the words to its base form\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjtkFR6Qz-D",
        "outputId": "99c82d16-d482-4777-e51f-0b0e154a6aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TfidfVectorizer()\n"
          ]
        }
      ],
      "source": [
        "# create vectorize data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer().fit(x_final)\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QukV9M1ldhF1"
      },
      "outputs": [],
      "source": [
        "# define the function to check the spellings\n",
        "def spell_checker(sentence):\n",
        "\n",
        "  # this list is for words which is to be ingnored\n",
        "  ignore_words = []\n",
        "\n",
        "  # open the file and load the ingnore words\n",
        "  try:\n",
        "    file = open(\"words.txt\",'r')\n",
        "    for word in file.readlines():\n",
        "      ignore_words.append(word[:-1])\n",
        "  except FileNotFoundError:\n",
        "    print(\"Creating new word.txt file....\")\n",
        "\n",
        "  # tokenize the sentence and corret it\n",
        "  token = nltk.word_tokenize(sentence)\n",
        "  for word in token: \n",
        "    corrected = TextBlob(word)\n",
        "    corrected = str(corrected.correct())\n",
        "\n",
        "    # if word is incorrect then give warring to user\n",
        "    if word not in ignore_words:\n",
        "      if word != corrected:\n",
        "        print(f\"Warning: {word} word looks incorrect\")\n",
        "        print(\"sugetion: \",corrected)\n",
        "        print(\"\")\n",
        "        a = input(\"if you found this suggeion helpful please enter Y ans N if not: \")\n",
        "\n",
        "        # if word is is correct then store it in file\n",
        "        if a=='N':\n",
        "          ignore_words.append(word)\n",
        "          file = open('words.txt','a+')\n",
        "          file.write(str(word)+\"\\n\")\n",
        "          file.close()\n",
        "          return sentence\n",
        "    \n",
        "\n",
        "        # if word is incorrect then ask user to enter question again\n",
        "        elif a==\"Y\":\n",
        "          sentence = input(\"try to enter question again\\n\")\n",
        "          return responce(sentence)\n",
        "\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN9IdtPbzJlZ"
      },
      "outputs": [],
      "source": [
        "# define the function to input the sentence and preprocess it\n",
        "def input_sentence(sentence):\n",
        "\n",
        "  if sentence != \"\":\n",
        "    sentence = spell_checker(sentence)\n",
        "\n",
        "  # removing any abnormal words in sentence\n",
        "  clean_sentece = re.sub('[^a-zA-Z\\s\\w]','',sentence)\n",
        "\n",
        "  # processing the sentence\n",
        "  sentence_words = nltk.word_tokenize(clean_sentece)\n",
        "  processed_sentence = []\n",
        "  for w in sentence_words:\n",
        "    if w not in stopwords:\n",
        "      word = stemmer.stem(w.lower())\n",
        "      processed_sentence.append(word)\n",
        "  processed_sentence = ' '.join(processed_sentence)\n",
        "\n",
        "  print(processed_sentence)\n",
        "  return processed_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMh19cM7zOBZ"
      },
      "outputs": [],
      "source": [
        "# defining the function to indentify the intent of the input\n",
        "def identify_intent(sentence):\n",
        "  \n",
        "  # predict the results using ANN model\n",
        "  result = model.predict(vect.transform([input_sentence(sentence)]).toarray())\n",
        "\n",
        "  # printing the max score\n",
        "  max_score = max(result[0])\n",
        "  print(max_score)\n",
        "\n",
        "  # if the score is more than thresh hold then process it future\n",
        "  if max_score >= 0.25:\n",
        "    index = 0\n",
        "\n",
        "    # getting the index of the intent to get the appropriate responce\n",
        "    for score in result:\n",
        "      for prob in score:\n",
        "        index +=1\n",
        "        if prob == max_score:\n",
        "          break\n",
        "    return index\n",
        "  \n",
        "  # if the score is less than threshold then ask user new question\n",
        "  elif max_score <= 0.25:\n",
        "    print(\"\"\"Soory but i don't have knowledge about it. I will let you know i about this is some time\"\"\");\n",
        "\n",
        "    # store the question to file to prvide knowledge avout it later\n",
        "    file = open('questions.txt','a+')\n",
        "    file.write(str(sentence)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    # ask the user new question\n",
        "    return identify_intent(input(\"Please try another question: \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cjPkTDezRkR"
      },
      "outputs": [],
      "source": [
        "# define the function to fetch the responce\n",
        "def responce(sentence):\n",
        "\n",
        "  # detect the tag using its index to fetch the proper respnce\n",
        "  output_tag = identify_intent(sentence)\n",
        "  print(\"hi\",output_tag)\n",
        "  result = tags[output_tag-1]\n",
        "\n",
        "  # select appropriate responce and show it\n",
        "  for intent in intents['Intents']:\n",
        "    if intent['tag'] == result:\n",
        "      print(intent['tag'])\n",
        "      responce = random.choice(intent['responses'])\n",
        "      break\n",
        "  return responce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TonHJW45zTRo",
        "outputId": "b0442522-c032-4894-e624-7d9152993715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "please 'enter' to exit form the conversation\n",
            "Warning: coviad word looks incorrect\n",
            "sugetion:  copied\n",
            "\n",
            "coviad\n",
            "0.40747043\n",
            "hi 567\n",
            "thanks\n",
            "chatbot: My pleasure\n",
            "----------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "a = \"a\"\n",
        "print(\"please 'enter' to exit form the conversation\")\n",
        "while a!='':\n",
        "  a = input(\"question: \")\n",
        "  if a=='':\n",
        "    break\n",
        "  output = responce(a)\n",
        "  print(\"chatbot:\", output)\n",
        "  print(\"----------------------------------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM2Rrnm1SvkF"
      },
      "outputs": [],
      "source": [
        "# use of LSTM model\n",
        "# voice input output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3.model_testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}